---
title: "Homework 2"
author: "Eunseo Kang"
output:
  html_document:
    toc: true
    toc_float: true
    code_folding: show

---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE, message = FALSE,
                      warning = FALSE)
# install.packages("tidymodels")
library(tidyverse) #collection of open source packages
library(tidymodels) #collection of packages for modeling and machine learning using tidyverse principles
tidymodels_prefer()
data <- read.csv('data/abalone.csv')
```



### Question 1

Your goal is to predict abalone age, which is calculated as the number of rings plus 1.5. Notice there currently is no `age` variable in the data set. Add `age` to the data set.

```{r}
data <- data %>% 
  mutate(age=rings+1.5)
head(data)
```

Assess and describe the distribution of `age`.
```{r}
data %>% 
  ggplot(aes(x=age)) +
  geom_histogram() +
  theme_bw()
```

* The distribution of abalone's age is centered around 10 and right-skewed, meaning that much of the ass of its distribution is at the lower end. Most observations range from age 5 to 20 and there are a few observations above 20.

### Question 2

Split the abalone data into a training set and a testing set. Use stratified sampling. You should decide on appropriate percentages for splitting the data.

*Remember that you'll need to set a seed at the beginning of the document to reproduce your results.*
```{r}
set.seed(910514)
data_split <- initial_split(data, prop = 0.80,
                                strata = age)
data_train <- training(data_split)
data_test <- testing(data_split)
```

### Question 3

Using the **training** data, create a recipe predicting the outcome variable, `age`, with all other predictor variables. Note that you should not include `rings` to predict `age`. Explain why you shouldn't use `rings` to predict `age`.

Steps for your recipe

1.  dummy code any categorical predictors

2.  create interactions between

    -   `type` and `shucked_weight`,
    -   `longest_shell` and `diameter`,
    -   `shucked_weight` and `shell_weight`

3.  center all predictors, and

4.  scale all predictors.

You'll need to investigate the `tidymodels` documentation to find the appropriate step functions to use.

```{r}
recipe <- recipe(age ~ ., data = data_train%>% select(-rings)) %>% 
  step_dummy(all_nominal_predictors()) %>% 
  step_interact(terms= ~ shucked_weight:starts_with("type")+longest_shell:diameter+shucked_weight:shell_weight) %>% 
  step_normalize(all_numeric_predictors())
```


* The reason why I should not include `rings` to predict `age` is that they are perfectly correlated and including it will break the model.

### Question 4

Create and store a linear regression object using the `"lm"` engine.
```{r}
lm_model <- linear_reg() %>% 
  set_engine("lm")
```


### Question 5

Now

1.  set up an empty workflow,
2.  add the model you created in Question 4, and
3.  add the recipe that you created in Question 3.

```{r}
lm_wflow <- workflow() %>% 
  add_model(lm_model) %>% 
  add_recipe(recipe)
```

### Question 6

Use your `fit()` object to predict the age of a hypothetical female abalone with longest_shell = 0.50, diameter = 0.10, height = 0.30, whole_weight = 4, shucked_weight = 1, viscera_weight = 2, shell_weight = 1.

```{r}
lm_fit <- fit(lm_wflow, data_train%>% select(-rings))
lm_fit %>% 
  # This returns the parsnip object:
  extract_fit_parsnip() %>% 
  # Now tidy the linear model object:
  tidy()

#make a data to predict
new_data <- data.frame(type="F", longest_shell=0.5, diameter=0.10, height=0.30, whole_weight=4, shucked_weight=1, viscera_weight=2, shell_weight=1)
train_res <- predict(lm_fit, new_data)
head(train_res)
```

* The predicted age is 22.8154.

### Question 7

Now you want to assess your model's performance. To do this, use the `yardstick` package:

1.  Create a metric set that includes *R^2^*, RMSE (root mean squared error), and MAE (mean absolute error).
2.  Use `predict()` and `bind_cols()` to create a tibble of your model's predicted values from the **training data** along with the actual observed ages (these are needed to assess your model's performance).
3.  Finally, apply your metric set to the tibble, report the results, and interpret the *R^2^* value.

```{r}
#install.packages("yardstick")
metrics <- metric_set(rsq, rmse, mae)

results <- predict(lm_fit, data_train%>% select(-rings, -age))
results <- bind_cols(results, data_train %>% select(age))
head(results)

metrics(results, truth = age, estimate = .pred)

```

* $R^2$ is 0.56, meaning that predictors can explain 56% of the variation of ages. 

### Required for 231 Students

In lecture, we presented the general bias-variance tradeoff, which takes the form:

$$
E[(y_0 - \hat{f}(x_0))^2]=Var(\hat{f}(x_0))+[Bias(\hat{f}(x_0))]^2+Var(\epsilon)
$$


#### Question 8

Which term(s) in the bias-variance tradeoff above represent the reproducible error? Which term(s) represent the irreducible error?

* Terms $Var(\hat{f}(x_0))+[Bias(\hat{f}(x_0))]^2$ are the reproducible error and the term $Var(\epsilon)$ is the irreducible error.

#### Question 9

Using the bias-variance tradeoff above, demonstrate that the expected test error is always at least as large as the irreducible error.

* Since $Var(\hat{f}(x_0))=E((\hat{f}(x_0)-E(\hat{f}(x_0)))^2)\geq 0$ and $[Bias(\hat{f}(x_0))]^2 \geq 0$ because they are the squared terms, 

$$Var(\hat{f}(x_0))+[Bias(\hat{f}(x_0))]^2 = E[(y_0 - \hat{f}(x_0))^2] - Var(\epsilon) \geq 0 $$
$$\therefore E[(y_0 - \hat{f}(x_0))^2]\geq Var(\epsilon) $$ 

#### Question 10

Prove the bias-variance tradeoff.


*
\begin{align}
E[(y_0 - \hat{f}(x_0))^2] & = E[(f(x_0)+\epsilon - \hat{f}(x_0))^2]& \\
& = E[(f(x_0)- \hat{f}(x_0))^2]+E(\epsilon^2)+2E((f(x_0)- \hat{f}(x_0))\epsilon)& \\
& = E[(f(x_0)- \hat{f}(x_0))^2]+E(\epsilon^2)+2(f(x_0)- \hat{f}(x_0))E(\epsilon|x) &(\text{by Law of Iterative expectation}) \\
& = E[(f(x_0)- \hat{f}(x_0))^2]+E(\epsilon^2) &(E(\epsilon|x)=0 \text{since the error term and x is independent}) \\
& = E[(f(x_0)- E(\hat{f}(x_0)) + E(\hat{f}(x_0)) - \hat{f}(x_0))^2] + Var(\epsilon) &\\
& = E[(E(\hat{f}(x_0))-f(x_0))^2]+E[(\hat{f}(x_0)-E(\hat{f}(x_0)))^2]-2E[(f(x_0)- E(\hat{f}(x_0)))(E(\hat{f}(x_0)) - \hat{f}(x_0))] + Var(\epsilon) &\\
& = (E(\hat{f}(x_0))-f(x_0))^2+E[(\hat{f}(x_0)-E(\hat{f}(x_0)))^2]-2(f(x_0)- E(\hat{f}(x_0)))E[(E(\hat{f}(x_0)) - \hat{f}(x_0))] + Var(\epsilon) & f(x_0)- E(\hat{f}(x_0)) \text{is just a constant}) \\
& = (E(\hat{f}(x_0))-f(x_0))^2+E[(\hat{f}(x_0)-E(\hat{f}(x_0)))^2]-2(f(x_0)- E(\hat{f}(x_0)))(E(\hat{f}(x_0)) - E(\hat{f}(x_0))) + Var(\epsilon) &  \\
& = (E(\hat{f}(x_0))-f(x_0))^2+E[(\hat{f}(x_0)-E(\hat{f}(x_0)))^2]+ Var(\epsilon) &  \\
& = Bias(\hat{f}(x_0))^2+Var(\hat{f}(x_0))+ Var(\epsilon)
\end{align}

