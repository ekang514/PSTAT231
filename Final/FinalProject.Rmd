---
title: "Final Project: Prediction of a Child's Health Status"
author: "Eunseo Kang"
date: "2022-12-11"
output: 
  html_document:
    number_sections: true
    code_folding: show

---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE, message = FALSE,
                      warning = FALSE)
```

![](image.png)

```{r}
library(janitor)
library(tidymodels)
library(tidyverse)
library(glmnet)
library(ggplot2)
library(readr)
library(corrr)
library(corrplot)
library(naniar)
library(visdat)
library(dplyr)
library(tune)
library(rpart.plot)
library(randomForest)
library(vip)
library(xgboost)
library(parsnip)
#install.packages("kknn")
library(kknn)
tidymodels_prefer()
set.seed(514222)
```
# Introduction

It is the core of interest for most parents, policy makers, and even for a child herself that how one individual's health outcomes are affected by various factors from home environment. In this project, I would like to predict a child's health status using predictors that describes family backgrounds using NHIS(National Health Interview Survey) ranging from 1999-2018.  

My main outcome of interest is the health status of a child, which is a categorical variable. It is either excellent, very good, good, fair, or poor. There are less than 1000 missing data in this outcome, and I dropped them. 

# Data 

I restricted the sample to those who are under age 18 to predict a child who resides with either parent. The number of sample observations are 328,312. I have 18 predictors and they are year, sex, age, race, mother's marital status, father's marital status, number of persons in family, parent present in the family, education of mother, education of father, US citizenship, educational attainment, above or below poverty threshold, total combined family income, any family member received food stamp, home ownership, health insurance coverage status, whether received special education or early intervention services. First, I import this NHIS data.

```{r}
data <- read.csv('data/health.csv',sep = ',') %>% 
  clean_names()
data <- na_if(data,'')
summary(data)
```

## Exploratory Data Analysis
Before doing modeling, EDA is done to see to have some sense how the data and distribution looks like, whether there are missing values in some variables, and whether there are any correlations.

First, I want to look at the distribution of the main outcome of interest. Health Status 'excellent' shows the most frequency and the frequency decreases as the health status gets worse. 

```{r}
data %>% 
  ggplot(aes(x =fct_infreq(health))) +
  geom_bar() +
  coord_flip() +
  labs(x="Health Status")
```

There are a few numerical variables such as year, age, and family size. Therefore, I draw a correlation plot to see the correlation between them. The correlation graph shows there is no relationship between these numerical variables, although I see very small positive relationship between age and family size. It makes sense since the more you are aged, the more likely you have another sibling. 
```{r}
data %>% 
  select(is.numeric) %>% 
  cor() %>% 
  corrplot(type = "lower")
```

Next, I also want to see whether there are patterns in age and family size in different health outcome groups. Therefore, I decided to draw a box plot by categories of health outcomes. The results show that there are not much difference in age and family size distribution in different health outcomes.

```{r}
data %>%
  ggplot() + 
    geom_boxplot(mapping=aes(x = health, y = age))

data %>%
  ggplot() + 
    geom_boxplot(mapping=aes(x = health, y = famsize))
```

Now, I check the missing data patterns on the training set.
```{r}
vis_dat(data, warn_large_data=F)
```

Thankfully, the proportion of missing observations is not big in each variable. Since all variables with missing data are categorical variable, I thought imputing can cause bias. Therefore, rather than imputing, I decided to change this missing value to zero since anyway I will include them as dummies in the prediction models. Since my main outcome health status is factor and year is also going to be considered as factor, I transformed the type of variable to factor.

```{r}
data[is.na(data)] <- 0
summary(data)
data$health <- as.factor(data$health)
data$year <- as.factor(data$year)
```


## Data Splitting

I divide data into training and test sets with the proportion of 70% for the training set. I stratify the sample since the distribution of health status is skewed.

```{r}
data_split <- data %>% 
  initial_split(strata = health, prop = 0.7)
data_train <- training(data_split)
data_test <- testing(data_split)
dim(data_train)
dim(data_test)
```

I can check that the data is appropriately divided into training and test sets by checking the dimensions.

# Model Building and Results

## Building Recipe

When building a recipe, I create dummies for all nominal predictors. 

```{r}
recipe <- recipe(health ~ ., data=data) %>% 
  step_dummy(all_nominal_predictors()) 
```

## Cross-validation to fold training set

I create stratified CV using 2 folds with repeats. The reason why I chose small number of folds is that it takes too long time due to large number of samples. I stratified the folds by health status.
```{r}
data_folds <- vfold_cv(data_train, strata = health, 
                          v = 2)
```

## Fitting model: Elastic net, Classification Tree models, K-nearest neighbors, and boosted tree model.

In this section, I fit the data to four models: Elastic net, Classification Tree models, K-nearest neighbors, and boosted tree model. Based on the result from this section, I will choose the model showing the highest ROC_AUC of prediction. Initially, I wanted to show the results of KNN model and boosted tree model, it took too much time, so I only include codes for them. 

### Elastic Net

When fitting and tuning the elastic net model, I will tune parameters for penalty. I also set 5 level to save time. 

```{r}
elastic_net_spec <- multinom_reg(penalty = tune()) %>% 
  set_mode("classification") %>% 
  set_engine("glmnet")

elastic_net_workflow <- workflow() %>% 
  add_recipe(recipe) %>% 
  add_model(elastic_net_spec)

grid <- grid_regular(penalty(range = c(-5, -1)), levels = c(penalty = 5))
```


Now, I will fit the model using tune_grid(). 

```{r, eval=FALSE}
tune_res <- tune_grid(
  elastic_net_workflow,
  resamples = data_folds, 
  grid = grid
)

save(tune_res, file = "model1.rda")

```

To see what values of penalty produce better result, I draw a plot using autoplot. 

```{r}
load(file = "model1.rda")
autoplot(tune_res)
```

I will select the best model using select_best and fit the model. I also save the result separately since it takes long time.

```{r}
best1 <- select_best(tune_res)
elastic_net_final <- finalize_workflow(elastic_net_workflow, best1)
```

```{r, eval=F}
elastic_net_final_fit <- fit(elastic_net_final, data = data_train)
save(elastic_net_final_fit, file="model1fit.rda")

```

Finally, to compare with the other model, I calculate the overall ROC_AUC on the training set. The ROC_AUC from the elastic net model is 0.6432633.

```{r}
load(file="model1fit.rda")
augment(elastic_net_final_fit, new_data = data_train) %>% 
  roc_auc(health, .pred_excellent:.pred_verygood) 
```

### Classification Tree

My second model is Classification Tree model. 
As the first step, I create a general decision tree specification as the engine with the mode of classification.

```{r}
tree_spec <- decision_tree() %>%
  set_engine("rpart")

class_tree_spec <- tree_spec %>%
  set_mode("classification")
```

Now, I tune the cost_complexity of the decision tree to find an optimal complexity. I set up the workflow including the process of tuning.

```{r}
class_tree_wf <- workflow() %>%
  add_model(class_tree_spec %>% set_args(cost_complexity = tune())) %>%
  add_recipe(recipe)

grid2 <- grid_regular(cost_complexity(range = c(-5, -1)), levels = c(cost_complexity = 5))
```

Now I should show which values of cost_complexity appear to produce the highest ROC_AUC.

```{r, eval=F}
tune_res2 <- tune_grid(
  class_tree_wf, 
  resamples = data_folds, 
  grid = grid2 
)

save(tune_res2, file = "model2.rda")
```

```{r}
load(file="model2.rda")

autoplot(tune_res2)
```

I choose the best complexity, and finalize the workflow by updating the value and the fit the model with the full training data set.

```{r}
best2 <- select_best(tune_res2)

class_tree_final <- finalize_workflow(class_tree_wf, best2)
```

```{r, eval=F}
class_tree_final_fit <- fit(class_tree_final, data = data_train)
save(class_tree_final_fit, file="model2fit.rda")
```

Lastly, I visualize this model.

```{r}
load(file="model2fit.rda")
class_tree_final_fit %>%
  extract_fit_engine() %>%
  rpart.plot()
```

To compare with the other models, I derive the ROC_AUC based on the training model. The ROC_AUC from the classification tree model is 0.5878494.

```{r}
augment(class_tree_final_fit, new_data = data_train) %>%
  roc_auc(health, .pred_excellent:.pred_verygood)
```


### K Nearest Neighbors Regression Model

Here, I apply K Nearest Neighbors Regression Model. First, I set up the specification including the tuning process for neighbors.

```{r}
knn_spec <- 
  nearest_neighbor() %>%
  set_args(neighbors = tune()) %>% 
  set_engine(engine = 'kknn') %>%
  set_mode('classification')
```

Next, I set up the workflow adding the model and recipe.
```{r}
knn_wkflow <- workflow() %>% 
  add_recipe(recipe) %>% 
  add_model(knn_spec)

grid3 <- grid_regular(neighbors(range = c(1,5)), levels = 2)
```

Now, I tune the model with the grid ranging from 1 to 5 with the level of 2. I chose this small level since I had massive data and it took too much time. After running many hours, I actually failed to finish running other two models. I think I should either need more time, or decrease the number of samples. I include the codes that runs properly, but do not evaluate them to create the document.

```{r, eval=F}
tune_knn <- tune_grid(
  knn_wkflow, 
  resamples = data_folds, 
  grid = grid3
)
save(tune_knn, file="kknn.rda")
```

```{r, eval=F}
load(file="kknn.rda")
autoplot(tune_knn)
```

I choose the best neighbors, and finalize the workflow by updating the value and the fit the model with the full training data set.
```{r, eval=F}
bestknn <- select_best(tune_knn)

knn_final <- finalize_workflow(knn_wkflow, bestknn)
```

```{r, eval=F}
knn_final_fit <- fit(knn_final, data = data_train)
save(knn_final_fit, file="knnfit.rda")
```

To compare with the other models, I derive the ROC_AUC based on the training model. 

```{r, eval=F}
load(file="knnfit.rda")
augment(knn_final_fit, new_data = data_train) %>%
  roc_auc(health, .pred_excellent:.pred_verygood)
```


### Boosted Tree

As the last model, I use Boosted tree model. I set up the specification including the tuning process for min_n.

```{r}
boost_spec <- boost_tree(min_n = tune()) %>%
  set_engine("xgboost") %>%   
  set_mode("classification")
```

Next, I set up the workflow adding the model. I again chose the small level to save time.

```{r}
boost_wf <- workflow() %>% 
  add_recipe(recipe) %>% 
  add_model(boost_spec)

grid4 <- grid_regular(min_n(range = c(2, 40)), levels=2)
```

Now I should show which values of tree appear to produce the highest ROC_AUC.

```{r, eval=F}
tune_res4 <- tune_grid(
  boost_wf, 
  resamples = data_folds, 
  grid = grid4
)

save(tune_res4, file="model4.rda")
```

```{r, eval=F}
load(file="model4.rda")
autoplot(tune_res4)
```

I choose the best mtry, and finalize the workflow by updating the value and the fit the model with the full training data set.

```{r, eval=F}
best4 <- select_best(tune_res4)

boost_final <- finalize_workflow(boost_wf, best4)
```

```{r, eval=F}
boost_final_fit <- fit(boost_final, data = data_train)
save(boost_final_fit, file="model4fit.rda")
```


To compare with the other models, I derive the ROC_AUC based on the training model. 

```{r, eval=F}
load(file="model4fit.rda")
augment(boost_final_fit, new_data = data_train) %>%
  roc_auc(health, .pred_excellent:.pred_verygood)
```


# Results of the Best Model and Performance

Unfortunately, I could only compare elastic net model and classification tree model. Among them, the one that shows the highest ROC_AUC is elastic net model. Therefore, I fit this model to test set.
```{r}
augment(elastic_net_final_fit, new_data = data_test) %>%
  accuracy(truth = health, estimate = .pred_class)

augment(elastic_net_final_fit, new_data = data_test) %>%
  roc_auc(health, .pred_excellent:.pred_verygood)

predicted_data <- augment(elastic_net_final_fit, new_data = data_test) %>% 
  select(health, starts_with(".pred"))

predicted_data %>% 
  conf_mat(truth = health, estimate = .pred_class) %>%
  autoplot(type = "heatmap")
```

The final model shows the accuracy of 0.5778364, and ROC_AUC of 0.6352697. The heatmap shows that it predicts 'excellent' very well, but tend to overestimate the health status even when the truth value if just 'very good' or 'good'. However, it was interesting that the model is differentiating 'fair' and 'poor' more meaningfully from better health status, although it was still overestimating the health status to 'excellent'. I guess this is due to that already imbalanced health outcomes skewed toward 'excellent' from the start. 

I can also plot ROC curves, one per level of the outcome. 

```{r}
augment(elastic_net_final_fit, new_data = data_test)%>%
  roc_curve(health, .pred_excellent:.pred_verygood) %>%
  autoplot()
```

In the ROC curves, the result shows that the model is worst at identifying 'very good' health status, while best at identifying 'poor', 'fair', and 'good' in order. Surprisingly, the model was not that good at identifying 'excellent' since it predicts other truth values to 'excellent' too often. 


# Conclusion
Throughout the research, the best model to predict a child's health status using information about family background is a elastic net model, but it was not perfect. It was an interesting analysis learning that I could predict a child's health status based on family background information. Although it makes sense intuitively, since we expect better home environment will guarantee better health for children, checking whether prediction actually works with the real data was fascinating experience. 

One thing I thought it would have been better was that the result could have been more accurate if I used any quantitative data about health status. Unfortunately, my data source was concentrated in qualitative data since it was survey data. There was variables about bmi, height, weight, but they all had missing values for young children. I think supplementing such data would be a good idea to proceed further research in this topic and can give more insights for predicting more important information about a child's health status.

